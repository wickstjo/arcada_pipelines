nameOverride: "kuberay"

# CAN BE CHANGED TO LOADBALANCER
service:
    type: ClusterIP

# CLUSTER HEAD
head:
    image:
        repository: rayproject/ray
        tag: 2.38.0-py312
    rayStartParams:
        dashboard-host: '0.0.0.0'
    resources:
        limits:
            cpu: "1"
            # To avoid out-of-memory issues, never allocate less than 2G memory for the Ray head.
            memory: "2G"
            nvidia.com/gpu: 0
        requests:
            cpu: "1"
            memory: "2G"
            nvidia.com/gpu: 0
    volumes:
        -   name: log-volume
            emptyDir: {}
    volumeMounts:
        -   mountPath: /tmp/ray
            name: log-volume
    # containerEnv:
    #     -   name: RAY_GRAFANA_IFRAME_HOST
    #         value: http://193.167.37.127:3000
    #     -   name: RAY_GRAFANA_HOST
    #         value: http://prometheus-grafana.prometheus-system.svc:80
    #     -   name: RAY_PROMETHEUS_HOST
    #         value: http://prometheus-kube-prometheus-prometheus.prometheus-system.svc:9090
    # env:
    #     -   name: RAY_GRAFANA_IFRAME_HOST
    #         value: http://193.167.37.127:3000
    #     -   name: RAY_GRAFANA_HOST
    #         value: http://prometheus-grafana.prometheus-system.svc:80
    #     -   name: RAY_PROMETHEUS_HOST
    #         value: http://prometheus-kube-prometheus-prometheus.prometheus-system.svc:9090


# DISABLE CLUSTER DEFAULT WORKERGROUP
worker:
    disabled: true

image:
    repository: ray_with_torch
    tag: latest
    pullPolicy: IfNotPresent

    # repository: rayproject/ray
    # pullPolicy: IfNotPresent

    # SELECT AN IMAGE TAG THAT SUPPORTS:
    #   1. GPUS
    #   2. PYTHON 3.121
    # tag: 2.38.0-py312-gpu

# REVERSE EACH CLUSTER NODE MANUALLY
# ONE WORKER PER NODE -- RESERVING ~90% OF ITS RESOURCES
additionalWorkerGroups:

    # 4090 MACHINE
    worker-4090:
        replicas: 1
        resources:
            limits:
                cpu: "28"
                memory: "40Gi"
                nvidia.com/gpu: "1"
            requests:
                cpu: "28"
                memory: "40Gi"
                nvidia.com/gpu: "1"
        affinity:
            nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                        - matchExpressions:
                            -   key: kubernetes.io/hostname
                                operator: In
                                values:
                                    - 4090-ubu

    # # 3080 MACHINE
    # worker-3080:
    #     replicas: 1
    #     resources:
    #         limits:
    #             cpu: "20"
    #             memory: "40Gi"
    #             nvidia.com/gpu: "1"
    #         requests:
    #             cpu: "20"
    #             memory: "40Gi"
    #             nvidia.com/gpu: "1"
    #     affinity:
    #         nodeAffinity:
    #             requiredDuringSchedulingIgnoredDuringExecution:
    #                 nodeSelectorTerms:
    #                     - matchExpressions:
    #                         -   key: kubernetes.io/hostname
    #                             operator: In
    #                             values:
    #                                 - 3080-ubu

    # DUAL-TITANS MACHINE
    # SPLIT RESOURCES INTO TWO WORKERS -- ONE GPU EACH
    worker-titans:
        replicas: 2
        resources:
            limits:
                cpu: "3"
                memory: "13Gi"
                nvidia.com/gpu: "1"
            requests:
                cpu: "3"
                memory: "13Gi"
                nvidia.com/gpu: "1"
        affinity:
            nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                        - matchExpressions:
                            -   key: kubernetes.io/hostname
                                operator: In
                                values:
                                    - titans

    # worker-titans-2:
    #     replicas: 1
    #     resources:
    #         limits:
    #             cpu: "3"
    #             memory: "13Gi"
    #             nvidia.com/gpu: "1"
    #         requests:
    #             cpu: "3"
    #             memory: "13Gi"
    #             nvidia.com/gpu: "1"

    #     containerEnv:
    #         -   name: CUDA_VISIBLE_DEVICES
    #             value: "1"
    #     affinity:
    #         nodeAffinity:
    #             requiredDuringSchedulingIgnoredDuringExecution:
    #                 nodeSelectorTerms:
    #                     - matchExpressions:
    #                         -   key: kubernetes.io/hostname
    #                             operator: In
    #                             values:
    #                                 - titans